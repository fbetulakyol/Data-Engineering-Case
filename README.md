# Data-Engineering-Case

Dataset: Brazilian E-Commerce dataset

Task: Create an ETL pipeline with Airflow

Used technologies:
•	AWS S3 <br/>
•	Spark <br/>
•	SparkSQL <br/>
•	Airflow <br/>

Tasks are shown in below. <br/>

![image](https://user-images.githubusercontent.com/13195544/158159376-5c631513-b25e-4bb7-a11e-6cf16c4ddd97.png)

• Datasets were downloaded from AWS S3 and cleaned. <br/>
• The cleaned data was read with Spark and a query is written with SparkSQL. <br/>
• Query result converted to csv file and uploaded to AWS S3. <br/>
