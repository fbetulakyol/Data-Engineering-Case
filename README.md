# Data-Engineering-Case

Dataset: Brazilian E-Commerce dataset

Task: Create an ETL pipeline with Airflow

Used technologies:
•	AWS S3
•	Spark
•	SparkSQL
•	Airflow

Tasks are shown in below.

![image](https://user-images.githubusercontent.com/13195544/158159376-5c631513-b25e-4bb7-a11e-6cf16c4ddd97.png)

• Datasets were downloaded from AWS S3 and cleaned. 
• The cleaned data was read with Spark and a query is written with SparkSQL.
• Query result converted to csv file and uploaded to AWS S3.
